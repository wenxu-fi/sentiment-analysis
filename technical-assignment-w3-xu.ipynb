{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"$$\\textbf{\\Huge Technical Assignment W3}$$","metadata":{}},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Part1}$$","metadata":{}},{"cell_type":"code","source":"!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n!unzip imdb-dataset-of-50k-movie-reviews.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:33:28.446302Z","iopub.execute_input":"2025-02-12T12:33:28.446542Z","iopub.status.idle":"2025-02-12T12:33:30.670223Z","shell.execute_reply.started":"2025-02-12T12:33:28.446521Z","shell.execute_reply":"2025-02-12T12:33:30.668625Z"}},"outputs":[{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\nLicense(s): other\nDownloading imdb-dataset-of-50k-movie-reviews.zip to /kaggle/working\n 62%|████████████████████████▎              | 16.0M/25.7M [00:00<00:00, 166MB/s]\n100%|███████████████████████████████████████| 25.7M/25.7M [00:00<00:00, 211MB/s]\nArchive:  imdb-dataset-of-50k-movie-reviews.zip\n  inflating: IMDB Dataset.csv        \n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"$$Step 1$$","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n#file_path = 'IMDB Dataset.csv'\n\n# Load the IMDB dataset\nimdb_data = pd.read_csv(file_path)\n#imdb_data = imdb_data.sample(n=2000, random_state=42)\n# Display the first few rows of the dataset to verify\nimdb_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:44:02.740249Z","iopub.execute_input":"2025-02-15T08:44:02.740555Z","iopub.status.idle":"2025-02-15T08:44:05.254469Z","shell.execute_reply.started":"2025-02-15T08:44:02.740525Z","shell.execute_reply":"2025-02-15T08:44:05.253543Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"$$Step 2$$","metadata":{}},{"cell_type":"code","source":"imdb_data['label'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Retain only the 'review' and 'sentiment' columns\nimdb_data = imdb_data[['review', 'label']]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:44:13.002339Z","iopub.execute_input":"2025-02-15T08:44:13.002677Z","iopub.status.idle":"2025-02-15T08:44:13.021208Z","shell.execute_reply.started":"2025-02-15T08:44:13.002653Z","shell.execute_reply":"2025-02-15T08:44:13.020319Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    imdb_data[\"review\"], imdb_data[\"label\"], test_size=0.2, random_state=42\n)\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_texts, train_labels, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:44:45.614137Z","iopub.execute_input":"2025-02-15T08:44:45.614603Z","iopub.status.idle":"2025-02-15T08:44:45.631325Z","shell.execute_reply.started":"2025-02-15T08:44:45.614578Z","shell.execute_reply":"2025-02-15T08:44:45.630549Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"$$Step 3$$","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:44:51.914444Z","iopub.execute_input":"2025-02-15T08:44:51.914764Z","iopub.status.idle":"2025-02-15T08:44:55.971565Z","shell.execute_reply.started":"2025-02-15T08:44:51.914725Z","shell.execute_reply":"2025-02-15T08:44:55.970705Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nimport torch\nfrom datasets import Dataset\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:45:03.522321Z","iopub.execute_input":"2025-02-15T08:45:03.522619Z","iopub.status.idle":"2025-02-15T08:45:24.462507Z","shell.execute_reply.started":"2025-02-15T08:45:03.522594Z","shell.execute_reply":"2025-02-15T08:45:24.461788Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels.tolist()})\nval_dataset = Dataset.from_dict({\"text\": val_texts.tolist(), \"label\": val_labels.tolist()})\ntest_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"label\": test_labels.tolist()})\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\ntrain_dataset = train_dataset.remove_columns([\"text\"])\nval_dataset = val_dataset.remove_columns([\"text\"])\ntest_dataset = test_dataset.remove_columns([\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:45:49.250115Z","iopub.execute_input":"2025-02-15T08:45:49.250781Z","iopub.status.idle":"2025-02-15T08:49:57.660442Z","shell.execute_reply.started":"2025-02-15T08:45:49.250755Z","shell.execute_reply":"2025-02-15T08:49:57.659804Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09822997b8d74c08b2772902f860a03d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ced2b33e834dbabe968eb20cfb5205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22baffc263ae4bfc8f3098e35fa2a9d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e80b8bd47564999b736b0965687e143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dfd52dcc48c4208afeb82219045a30b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805d645b9bc741cd9ce0451382592002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6912cdbf3bb46f78ab047c27deb6dad"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:50:26.970293Z","iopub.execute_input":"2025-02-15T08:50:26.970603Z","iopub.status.idle":"2025-02-15T08:50:26.976636Z","shell.execute_reply.started":"2025-02-15T08:50:26.970583Z","shell.execute_reply":"2025-02-15T08:50:26.975797Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"$$Step 4$$","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nmodel=DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\",\n        num_labels=2,\n    )\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=1)\n    acc = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,  \n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_strategy=\"epoch\",\n    fp16=True,\n    disable_tqdm=True,\n    report_to=\"none\",  # disable wandb\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train and Evaluate\ntrainer.train()\nmetrics = trainer.evaluate() \nprint(\"Validation Metrics:\", metrics)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:51:29.462031Z","iopub.execute_input":"2025-02-15T08:51:29.462365Z","iopub.status.idle":"2025-02-15T09:08:24.024508Z","shell.execute_reply.started":"2025-02-15T08:51:29.462339Z","shell.execute_reply":"2025-02-15T09:08:24.023669Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27dabecaf1b4fb49cbadd54509f0ac7"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-9-d3b0e8dbfd60>:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2729, 'grad_norm': 267290.3125, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n{'eval_loss': 0.22469815611839294, 'eval_accuracy': 0.9115, 'eval_precision': 0.8912890349847382, 'eval_recall': 0.9393714427122, 'eval_f1': 0.9146987951807228, 'eval_runtime': 36.3515, 'eval_samples_per_second': 220.074, 'eval_steps_per_second': 6.877, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1278, 'grad_norm': 106537.2421875, 'learning_rate': 0.0, 'epoch': 2.0}\n{'eval_loss': 0.27358222007751465, 'eval_accuracy': 0.918875, 'eval_precision': 0.9223107569721115, 'eval_recall': 0.9166048007918832, 'eval_f1': 0.9194489263994042, 'eval_runtime': 36.3397, 'eval_samples_per_second': 220.145, 'eval_steps_per_second': 6.88, 'epoch': 2.0}\n{'train_runtime': 975.6825, 'train_samples_per_second': 65.595, 'train_steps_per_second': 2.05, 'train_loss': 0.20033306121826172, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.27358222007751465, 'eval_accuracy': 0.918875, 'eval_precision': 0.9223107569721115, 'eval_recall': 0.9166048007918832, 'eval_f1': 0.9194489263994042, 'eval_runtime': 36.388, 'eval_samples_per_second': 219.853, 'eval_steps_per_second': 6.87, 'epoch': 2.0}\nValidation Metrics: {'eval_loss': 0.27358222007751465, 'eval_accuracy': 0.918875, 'eval_precision': 0.9223107569721115, 'eval_recall': 0.9166048007918832, 'eval_f1': 0.9194489263994042, 'eval_runtime': 36.388, 'eval_samples_per_second': 219.853, 'eval_steps_per_second': 6.87, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Final evaluation on test set\ntest_metrics = trainer.evaluate(test_dataset)   \nprint(\"Test Set Metrics:\", test_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:09:09.353676Z","iopub.execute_input":"2025-02-15T09:09:09.353986Z","iopub.status.idle":"2025-02-15T09:09:56.149344Z","shell.execute_reply.started":"2025-02-15T09:09:09.353960Z","shell.execute_reply":"2025-02-15T09:09:56.148595Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.27002236247062683, 'eval_accuracy': 0.9167, 'eval_precision': 0.9152843601895735, 'eval_recall': 0.9198253621750347, 'eval_f1': 0.9175492427991685, 'eval_runtime': 46.7879, 'eval_samples_per_second': 213.731, 'eval_steps_per_second': 6.69, 'epoch': 2.0}\nTest Set Metrics: {'eval_loss': 0.27002236247062683, 'eval_accuracy': 0.9167, 'eval_precision': 0.9152843601895735, 'eval_recall': 0.9198253621750347, 'eval_f1': 0.9175492427991685, 'eval_runtime': 46.7879, 'eval_samples_per_second': 213.731, 'eval_steps_per_second': 6.69, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"$$Step 5$$","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"fine_tuned_DistilBert_Xu\")\ntokenizer.save_pretrained(\"fine_tuned_DistilBert_Xu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:10:45.416761Z","iopub.execute_input":"2025-02-15T09:10:45.417047Z","iopub.status.idle":"2025-02-15T09:10:46.085131Z","shell.execute_reply.started":"2025-02-15T09:10:45.417026Z","shell.execute_reply":"2025-02-15T09:10:46.084290Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_DistilBert_Xu/tokenizer_config.json',\n 'fine_tuned_DistilBert_Xu/special_tokens_map.json',\n 'fine_tuned_DistilBert_Xu/vocab.txt',\n 'fine_tuned_DistilBert_Xu/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:11:54.401608Z","iopub.execute_input":"2025-02-15T09:11:54.401910Z","iopub.status.idle":"2025-02-15T09:11:54.425160Z","shell.execute_reply.started":"2025-02-15T09:11:54.401885Z","shell.execute_reply":"2025-02-15T09:11:54.424278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35839b0339b94cc9a9592e0624efde5e"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom huggingface_hub import HfApi\n\n# Define the repo name \nrepo_name = \"Wenfi/fine-tuned-DistilBert_Xu\"\n\n# Push model to Hugging Face Hub\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:13:22.891663Z","iopub.execute_input":"2025-02-15T09:13:22.891956Z","iopub.status.idle":"2025-02-15T09:13:35.295090Z","shell.execute_reply.started":"2025-02-15T09:13:22.891935Z","shell.execute_reply":"2025-02-15T09:13:35.294455Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6b193acdc14e11af7c7e7a0ce3b7f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aff26e8dfaf4c4ca643dde7bcfdbff1"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Wenfi/fine-tuned-DistilBert_Xu/commit/484139a6642cf7c0b8746f2eacb416a0a11ef73b', commit_message='Upload tokenizer', commit_description='', oid='484139a6642cf7c0b8746f2eacb416a0a11ef73b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Wenfi/fine-tuned-DistilBert_Xu', endpoint='https://huggingface.co', repo_type='model', repo_id='Wenfi/fine-tuned-DistilBert_Xu'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Verify the model ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_loaded = AutoModelForSequenceClassification.from_pretrained(\"Wenfi/fine-tuned-DistilBert_Xu\")\ntokenizer_loaded = AutoTokenizer.from_pretrained(\"Wenfi/fine-tuned-DistilBert_Xu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:18:46.501501Z","iopub.execute_input":"2025-02-15T09:18:46.501823Z","iopub.status.idle":"2025-02-15T09:18:54.533472Z","shell.execute_reply.started":"2025-02-15T09:18:46.501799Z","shell.execute_reply":"2025-02-15T09:18:54.532606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb6bfff4c0a4326a4249611ddd9c845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32209d82a3844d8d9f2a975796da3bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"100c6298886441da83273df70a1be2c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7e2baec30e4a278affa05976db74b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7251c56f72b944be82aa8885144bca4c"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"inputs = tokenizer_loaded(\"This is a test sentence.\", return_tensors=\"pt\")\noutputs = model_loaded(**inputs)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T09:21:53.782346Z","iopub.execute_input":"2025-02-15T09:21:53.782659Z","iopub.status.idle":"2025-02-15T09:21:53.953721Z","shell.execute_reply.started":"2025-02-15T09:21:53.782636Z","shell.execute_reply":"2025-02-15T09:21:53.952969Z"}},"outputs":[{"name":"stdout","text":"SequenceClassifierOutput(loss=None, logits=tensor([[-0.1587,  0.4488]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":" Link to the fine tuned model: https://huggingface.co/Wenfi/fine-tuned-DistilBert_Xu","metadata":{}},{"cell_type":"markdown","source":"[Click here to view my model on Hugging Face](https://huggingface.co/Wenfi/fine-tuned-DistilBert_Xu)","metadata":{}},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Part 2}$$","metadata":{}},{"cell_type":"markdown","source":"$$Step6-7:app.py$$","metadata":{}},{"cell_type":"markdown","source":"Step 8:  Curl:\n curl -X POST \"http://127.0.0.1:8000/analyze/\" ^\n     -H \"Content-Type: application/json\" ^\n     -d \"{ \\\"text\\\": \\\"I love this product, it's amazing!\\\", \\\"model\\\": \\\"llama\\\" }\"\n\ncurl -X POST \"http://127.0.0.1:8000/analyze/\" ^\n     -H \"Content-Type: application/json\" ^\n     -d \"{ \\\"text\\\": \\\"I love this product, it's amazing!\\\", \\\"model\\\": \\\"custom\\\" }\"\n\n     Python: test_app.py\n","metadata":{}},{"cell_type":"markdown","source":"Step 9: Clear and reusable prompt for the Llama 3 model in Groq Cloud could be:\n\nDetermine the sentiment of this text as positive, negative, or neutral, and provide the confidence level for each sentiment, Please make sure to explain the reasoning behind the sentiment classification.: {text}\n","metadata":{}},{"cell_type":"markdown","source":"Step 10: \n\nUI address:http//localhost:5173/\nIf I type: This smovie is boring,  model: Llama 3, \nresult: Sentiment: the sentiment of this text is **negative**. the word \"boring\" has a negative connotation, indicating that the speaker did not enjoy the movie.UI address: \nConfidence: 0.9\n\nIf I choose model: Custom Model\nResult: Sentiment: LABEL_0\nConfidence: 0.9976978898048401\n\n\nStep 11-12: https://github.com/wenxu-fi/sentiment-analysis/","metadata":{}},{"cell_type":"markdown","source":"Step 13: ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}